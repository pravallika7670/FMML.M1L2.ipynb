{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravallika7670/FMML.M1L2.ipynb/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "40a48f78-066f-4d55-9ab5-3794bc7aee89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 20640\n",
            "\n",
            ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            ":Attribute Information:\n",
            "    - MedInc        median income in block group\n",
            "    - HouseAge      median house age in block group\n",
            "    - AveRooms      average number of rooms per household\n",
            "    - AveBedrms     average number of bedrooms per household\n",
            "    - Population    block group population\n",
            "    - AveOccup      average number of household members\n",
            "    - Latitude      block group latitude\n",
            "    - Longitude     block group longitude\n",
            "\n",
            ":Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. rubric:: References\n",
            "\n",
            "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "  Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "9b067dbc-7f59-4fcc-efd6-02ed6812b63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "149f8343-73bc-47e4-d150-a78addaff445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "8ade6fac-c3c9-4843-ca0a-6351acd72498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "05e931f3-9cb8-4d59-e8fc-731040114929"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "a10c8ac5-59f2-41d0-83cc-3c4ca3d7b79e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "4ffa6a34-eae1-4323-aad9-288b820d0b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "b3b2bcb4-71ff-42d4-e1dc-7f7f16c28551"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "To help you determine the best combination of features, I need more context. Could you clarify what features you're referring to? For example, are you asking about product features, software options, or characteristics in a specific context? The more details you provide, the better I can assist you.\n",
        "\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "Testing or visualizing four or more features depends on the type of data you're working with and the goals of your analysis. Here are some common methods:\n",
        "\n",
        "### 1. *Scatter Plot Matrix (Pair Plot)*\n",
        "   - *What it is:* A grid of scatter plots showing relationships between each pair of features.\n",
        "   - *When to use:* When you want to visualize pairwise relationships among features.\n",
        "   - *Tools:* Seaborn’s pairplot() or Pandas scatter_matrix().\n",
        "\n",
        "### 2. *Heatmap*\n",
        "   - *What it is:* A visual representation of data where individual values are represented by colors.\n",
        "   - *When to use:* To show the correlation between features.\n",
        "   - *Tools:* Seaborn’s heatmap().\n",
        "\n",
        "### 3. *Parallel Coordinates Plot*\n",
        "   - *What it is:* A plot that represents each feature as a vertical line, with each data point as a line connecting them.\n",
        "   - *When to use:* To visualize patterns and trends across multiple features.\n",
        "   - *Tools:* Matplotlib’s parallel_coordinates() or Plotly.\n",
        "\n",
        "### 4. *Principal Component Analysis (PCA)*\n",
        "- *What it is:* A dimensionality reduction technique that transforms high-dimensional data into a lower-dimensional space.\n",
        "   - *When to use:* To visualize how the features relate in a reduced dimension (2D or 3D).\n",
        "   - *Tools:* Scikit-learn’s PCA() or Seaborn's pairplot() after applying PCA.\n",
        "\n",
        "### 5. *Radial Plot (Spider Plot)*\n",
        "   - *What it is:* A plot that represents each feature as a spoke on a wheel, with data points plotted as a polygon.\n",
        "   - *When to use:* To compare multiple features at once.\n",
        "   - *Tools:* Matplotlib’s radar_chart().\n",
        "\n",
        "### 6. *3D Scatter Plot*\n",
        "   - *What it is:* A three-dimensional scatter plot where each axis represents one feature, and color or size can represent additional features.\n",
        "   - *When to use:* When you have three features to visualize simultaneously, and the fourth can be represented by color, size, or shape.\n",
        "   - *Tools:* Matplotlib, Plotly.\n",
        "\n",
        "### 7. *t-SNE or UMAP*\n",
        "   - *What it is:* Non-linear dimensionality reduction techniques that preserve local structures.\n",
        "   - *When to use:* When you want to visualize clusters or patterns in high-dimensional data.\n",
        "   - *Tools:* Scikit-learn, UMAP-learn.\n",
        "\n",
        "### 8. *Feature Importance Plot*\n",
        "   - *What it is:* A bar plot that shows the relative importance of each feature for a specific model.\n",
        "   - *When to use:* To determine which features contribute most to the model's predictions.\n",
        "   - *Tools:* Scikit-learn’s plot_importance() or XGBoost’s plot_importance().\n",
        "\n",
        "### 9. *Contour Plots*\n",
        "   - *What it is:* A plot that shows the joint distribution of two features with additional dimensions encoded in color or contour lines.\n",
        "   - *When to use:* To visualize how the distribution of one feature changes with respect to another while considering additional features.\n",
        "   - *Tools:* Matplotlib’s contour() or contourf().\n",
        "\n",
        "### 10. *Box Plot*\n",
        "   - *What it is:* A graphical representation of data distribution through quartiles.\n",
        "   - *When to use:* To compare distributions of multiple features.\n",
        "   - *Tools:* Seaborn’s boxplot().\n",
        "\n",
        "### Practical Example:\n",
        "If you have four features (say, A, B, C, and D), you could:\n",
        "1. Use a scatter plot matrix to see pairwise relationships.\n",
        "2. Apply PCA to reduce dimensionality and visualize in 2D.\n",
        "3. Use a heatmap to examine correlations.\n",
        "\n",
        "Would you like more specific examples or a walkthrough of one of these methods?\n",
        "\n",
        "\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "Certainly! Creating features, also known as \"feature engineering,\" is a critical part of data analysis and machine learning. Here are a few examples of how you might create new features based on existing data in different contexts:\n",
        "\n",
        "### 1. *E-Commerce:*\n",
        "   - *Existing Data:* Customer purchase history, product prices, and timestamps.\n",
        "   - *New Features:*\n",
        "     - *Average Order Value (AOV):* Average amount spent per order by each customer.\n",
        "     - *Days Since Last Purchase:* Time since the customer last made a purchase.\n",
        "     - *Purchase Frequency:* Number of purchases per unit of time (e.g., per month).\n",
        "     - *Discount Rate:* Difference between original price and purchase price, indicating sensitivity to discounts.\n",
        "\n",
        "### 2. *Real Estate:*\n",
        "   - *Existing Data:* Property size, location, number of rooms, and age of the property.\n",
        "   - *New Features:*\n",
        "     - *Price Per Square Foot:* Total price divided by the area of the property.\n",
        "     - *Room-to-Space Ratio:* Number of rooms divided by the square footage.\n",
        "     - *Age of Property in Years:* Current year minus the year the property was built.\n",
        "     - *Proximity to Amenities:* Distance to nearest school, park, or shopping center.\n",
        "\n",
        "### 3. *Health & Fitness:*\n",
        "   - *Existing Data:* Daily step count, calorie intake, hours of sleep, and workout type.\n",
        "   - *New Features:*\n",
        "- *Active vs. Sedentary Ratio:* Ratio of active minutes to sedentary minutes per day.\n",
        "     - *Calories Burned per Step:* Calculated using step count and personal metrics like weight.\n",
        "     - *Sleep Efficiency:* Ratio of actual sleep time to time spent in bed.\n",
        "     - *Workout Intensity Score:* Derived from the type of workout, duration, and heart rate.\n",
        "\n",
        "### 4. *Financial Analysis:*\n",
        "   - *Existing Data:* Stock prices, trading volume, and economic indicators.\n",
        "   - *New Features:*\n",
        "     - *Moving Average:* Average stock price over a specific period (e.g., 50-day moving average).\n",
        "     - *Price Momentum:* Rate of change in stock price over time.\n",
        "     - *Volatility Index:* Measure of the stock's price fluctuations over a period.\n",
        "     - *Price-to-Earnings Growth (PEG) Ratio:* Price-to-earnings ratio divided by the growth rate of earnings.\n",
        "\n",
        "### 5. *Social Media Analysis:*\n",
        "   - *Existing Data:* Number of likes, comments, shares, and post frequency.\n",
        "   - *New Features:*\n",
        "     - *Engagement Rate:* Total engagement (likes, comments, shares) divided by the number of followers.\n",
        "- *Post Impact Score:* Weighted score based on engagement and the reach of a post.\n",
        "     - *Time Between Posts:* Average time interval between posts.\n",
        "     - *Sentiment Score:* Derived from natural language processing (NLP) analysis of comments.\n",
        "\n",
        "### 6. *Retail:*\n",
        "   - *Existing Data:* Product sales data, customer demographics, and store locations.\n",
        "   - *New Features:*\n",
        "     - *Sales Growth Rate:* Rate at which sales are increasing or decreasing over time.\n",
        "     - *Customer Lifetime Value (CLV):* Predicted revenue a customer will generate over their relationship with the business.\n",
        "     - *Seasonal Index:* Sales performance adjusted for seasonal patterns.\n",
        "     - *Customer Segmentation:* Grouping customers based on purchasing behavior, demographics, or engagement.\n",
        "\n",
        "### 7. *Marketing:*\n",
        "   - *Existing Data:* Campaign budget, conversion rate, and ad impressions.\n",
        "   - *New Features:*\n",
        "     - *Return on Ad Spend (ROAS):* Revenue generated from ad campaigns divided by the ad spend.\n",
        "     - *Cost per Acquisition (CPA):* Total cost divided by the number of new customers acquired.\n",
        "     - *Click-Through Rate (CTR):* Number of clicks divided by the number of impressions.\n",
        "     - *Customer Journey Duration:* Time taken from the first interaction to purchase.\n",
        "\n",
        "Creating new features involves understanding the domain, identifying relationships or patterns in the data, and then crafting features that can help in making predictions or understanding the data better.\n",
        "\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n",
        "Yes, the features mentioned above can work for different classes beyond binary classes like 0 and 1. These features are generally applicable to multi-class classification, regression tasks, or other forms of analysis. Here’s how:\n",
        "\n",
        "### 1. *Multi-Class Classification:*\n",
        "   - *Example:* Predicting the category of a product (e.g., electronics, clothing, food).\n",
        "   - *Feature Application:*\n",
        "     - *Engagement Rate:* Useful across multiple product categories to see which types of products engage customers more.\n",
        "     - *Price Per Square Foot (Real Estate):* Can differentiate between luxury, mid-range, and budget properties.\n",
        "     - *Workout Intensity Score (Health & Fitness):* Can distinguish between different levels of workout intensity, which may correlate with various fitness goals or outcomes.\n",
        "\n",
        "### 2. *Regression Tasks:*\n",
        "   - *Example:* Predicting continuous outcomes, like house prices or stock returns.\n",
        "   - *Feature Application:*\n",
        "     - *Moving Average (Financial):* A good predictor of stock price trends, which is not limited to binary outcomes.\n",
        "     - *Customer Lifetime Value (Retail):* A continuous feature predicting revenue generated by a customer.\n",
        "     - *Sales Growth Rate (Retail):* Useful in predicting future sales, which is a continuous outcome.\n",
        "\n",
        "### 3. *Multi-Class or Ordinal Classification:*\n",
        "   - *Example:* Predicting academic grades (A, B, C, etc.) or customer satisfaction levels (Very Satisfied, Satisfied, Neutral, Unsatisfied, Very Unsatisfied).\n",
        "   - *Feature Application:*\n",
        "     - *Sentiment Score (Social Media):* Can disting\n",
        "     - *Sentiment Score (Social Media):* Can distinguish between various levels of customer satisfaction.\n",
        "     - *Proximity to Amenities (Real Estate):* Can affect property values across different classes (e.g., residential, commercial, industrial).\n",
        "     - *Cost per Acquisition (Marketing):* Can be used to analyze the effectiveness of different marketing channels, each being a different class.\n",
        "\n",
        "### 4. *Unsupervised Learning:*\n",
        "   - *Example:* Clustering customers into different segments.\n",
        "   - *Feature Application:*\n",
        "     - *Customer Segmentation (Retail):* Can be used to group customers without predefined classes, and the resulting clusters can be more than two.\n",
        "     - *Purchase Frequency (E-Commerce):* Useful for clustering customers based on how often they shop.\n",
        "     - *Time Between Posts (Social Media):* Can help in clustering users by their activity levels.\n",
        "\n",
        "### 5. *Anomaly Detection:*\n",
        "   - *Example:* Detecting fraudulent transactions in a multi-class setting.\n",
        "   - *Feature Application:*\n",
        "     - *Volatility Index (Financial):* Can indicate unusual fluctuations in stock prices, which might be labeled differently in a mulmight be labeled differently in a multi-class context (e.g., minor fluctuation, significant fluctuation, etc.).\n",
        "     - *Engagement Rate (Social Media):* Could flag unusual user activity that deviates from typical engagement patterns.\n",
        "\n",
        "### *Why These Features Work Beyond 0 and 1:*\n",
        "- *Scalability:* The features are continuous or categorical, making them adaptable to more than just binary classification.\n",
        "- *Generalizability:* These features capture inherent properties of the data (e.g., trends, ratios, frequencies) that apply to various classes, not just binary ones.\n",
        "- *Flexibility:* They can be used with different types of machine learning models (e.g.,\n",
        "decision trees, neural networks, clustering algorithms) that support multi-class and continuous output.\n",
        "\n",
        "### Adapting Features for Multi-Class Use:\n",
        "- *Normalization/Scaling:* Ensure features are scaled appropriately, especially when dealing with multiple classes with different ranges.\n",
        "- *Feature Selection:* Some features might be more relevant for certain classes, so using techniques like feature importance ranking can help refine them for multi-class problems.\n",
        "- *Transformation:* Sometimes, you may need to create additional features or transform existing ones to capture relationships unique to multi-class settings.\n",
        "\n",
        "In summary, the features you engineer should generally be applicable to different types of classes, not just binary ones. They should be tested and validated to ensure they provide meaningful insights or predictive power in the specific context you’re working with."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}